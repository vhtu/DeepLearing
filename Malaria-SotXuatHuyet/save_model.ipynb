{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"save_model.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"1T3JPv-VRZJA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"a80fa835-edf5-4577-f8d6-8bec576cb2de","executionInfo":{"status":"ok","timestamp":1554868196503,"user_tz":-420,"elapsed":22962,"user":{"displayName":"Võ Hoàng Tú","photoUrl":"","userId":"10850768615639726333"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"10QvVVIlVxd-","colab_type":"code","colab":{}},"cell_type":"code","source":["# import the necessary packages\n","from keras.layers.normalization import BatchNormalization\n","from keras.layers.convolutional import Conv2D\n","from keras.layers.convolutional import AveragePooling2D\n","from keras.layers.convolutional import MaxPooling2D\n","from keras.layers.convolutional import ZeroPadding2D\n","from keras.layers.core import Activation\n","from keras.layers.core import Dense\n","from keras.layers import Flatten\n","from keras.layers import Input\n","from keras.models import Model\n","from keras.layers import add\n","from keras.regularizers import l2\n","from keras import backend as K\n","\n","class ResNet:\n","\t@staticmethod\n","\tdef residual_module(data, K, stride, chanDim, red=False,\n","\t\treg=0.0001, bnEps=2e-5, bnMom=0.9):\n","\t\t# the shortcut branch of the ResNet module should be\n","\t\t# initialize as the input (identity) data\n","\t\tshortcut = data\n","\n","\t\t# the first block of the ResNet module are the 1x1 CONVs\n","\t\tbn1 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n","\t\t\tmomentum=bnMom)(data)\n","\t\tact1 = Activation(\"relu\")(bn1)\n","\t\tconv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,\n","\t\t\tkernel_regularizer=l2(reg))(act1)\n","\n","\t\t# the second block of the ResNet module are the 3x3 CONVs\n","\t\tbn2 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n","\t\t\tmomentum=bnMom)(conv1)\n","\t\tact2 = Activation(\"relu\")(bn2)\n","\t\tconv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\n","\t\t\tpadding=\"same\", use_bias=False,\n","\t\t\tkernel_regularizer=l2(reg))(act2)\n","\n","\t\t# the third block of the ResNet module is another set of 1x1\n","\t\t# CONVs\n","\t\tbn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n","\t\t\tmomentum=bnMom)(conv2)\n","\t\tact3 = Activation(\"relu\")(bn3)\n","\t\tconv3 = Conv2D(K, (1, 1), use_bias=False,\n","\t\t\tkernel_regularizer=l2(reg))(act3)\n","\n","\t\t# if we are to reduce the spatial size, apply a CONV layer to\n","\t\t# the shortcut\n","\t\tif red:\n","\t\t\tshortcut = Conv2D(K, (1, 1), strides=stride,\n","\t\t\t\tuse_bias=False, kernel_regularizer=l2(reg))(act1)\n","\n","\t\t# add together the shortcut and the final CONV\n","\t\tx = add([conv3, shortcut])\n","\n","\t\t# return the addition as the output of the ResNet module\n","\t\treturn x\n","\n","\t@staticmethod\n","\tdef build(width, height, depth, classes, stages, filters,\n","\t\treg=0.0001, bnEps=2e-5, bnMom=0.9):\n","\t\t# initialize the input shape to be \"channels last\" and the\n","\t\t# channels dimension itself\n","\t\tinputShape = (height, width, depth)\n","\t\tchanDim = -1\n","\n","\t\t# if we are using \"channels first\", update the input shape\n","\t\t# and channels dimension\n","\t\tif K.image_data_format() == \"channels_first\":\n","\t\t\tinputShape = (depth, height, width)\n","\t\t\tchanDim = 1\n","\n","\t\t# set the input and apply BN\n","\t\tinputs = Input(shape=inputShape)\n","\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n","\t\t\tmomentum=bnMom)(inputs)\n","\n","\t\t# apply CONV => BN => ACT => POOL to reduce spatial size\n","\t\tx = Conv2D(filters[0], (5, 5), use_bias=False,\n","\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n","\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n","\t\t\tmomentum=bnMom)(x)\n","\t\tx = Activation(\"relu\")(x)\n","\t\tx = ZeroPadding2D((1, 1))(x)\n","\t\tx = MaxPooling2D((3, 3), strides=(2, 2))(x)\n","\n","\t\t# loop over the number of stages\n","\t\tfor i in range(0, len(stages)):\n","\t\t\t# initialize the stride, then apply a residual module\n","\t\t\t# used to reduce the spatial size of the input volume\n","\t\t\tstride = (1, 1) if i == 0 else (2, 2)\n","\t\t\tx = ResNet.residual_module(x, filters[i + 1], stride,\n","\t\t\t\tchanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n","\n","\t\t\t# loop over the number of layers in the stage\n","\t\t\tfor j in range(0, stages[i] - 1):\n","\t\t\t\t# apply a ResNet module\n","\t\t\t\tx = ResNet.residual_module(x, filters[i + 1],\n","\t\t\t\t\t(1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n","\n","\t\t# apply BN => ACT => POOL\n","\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n","\t\t\tmomentum=bnMom)(x)\n","\t\tx = Activation(\"relu\")(x)\n","\t\tx = AveragePooling2D((8, 8))(x)\n","\n","\t\t# softmax classifier\n","\t\tx = Flatten()(x)\n","\t\tx = Dense(classes, kernel_regularizer=l2(reg))(x)\n","\t\tx = Activation(\"softmax\")(x)\n","\n","\t\t# create the model\n","\t\tmodel = Model(inputs, x, name=\"resnet\")\n","\n","\t\t# return the constructed network architecture\n","\t\treturn model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y8lClKhtSk7H","colab_type":"code","colab":{}},"cell_type":"code","source":["# set the matplotlib backend so figures can be saved in the background\n","import matplotlib\n","import sys\n","# import the necessary packages\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.optimizers import SGD\n","from sklearn.metrics import classification_report\n","from imutils import paths\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import argparse\n","import os"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pEcXvNDSV4rd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":90},"outputId":"5fa73156-ff16-43d1-b894-45565776ab9d","executionInfo":{"status":"ok","timestamp":1554867830940,"user_tz":-420,"elapsed":5790,"user":{"displayName":"Võ Hoàng Tú","photoUrl":"","userId":"10850768615639726333"}}},"cell_type":"code","source":["# initialize our Keras implementation of ResNet model and compile it\n","model = ResNet.build(64, 64, 3, 2, (2, 2, 3),\n","\t(32, 64, 128, 256), reg=0.0005)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"}]},{"metadata":{"id":"2O6wJpQVWE-N","colab_type":"code","colab":{}},"cell_type":"code","source":["model.summary()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"99wwdpQoWZqt","colab_type":"code","colab":{}},"cell_type":"code","source":["# initialize the number of training epochs and batch size\n","NUM_EPOCHS = 25\n","BS = 32\n"," \n","# derive the path to the directories containing the training,\n","# validation, and testing splits, respectively\n","TRAIN_PATH = os.path.sep.join([\"/content/gdrive/My Drive/app/malaria-SotXuatHuyet/malaria\", \"training\"])\n","VAL_PATH = os.path.sep.join([\"/content/gdrive/My Drive/app/malaria-SotXuatHuyet/malaria\", \"validation\"])\n","TEST_PATH = os.path.sep.join([\"/content/gdrive/My Drive/app/malaria-SotXuatHuyet/malaria\", \"testing\"])\n"," \n","# determine the total number of image paths in training, validation,\n","# and testing directories\n","totalTrain = len(list(paths.list_images(TRAIN_PATH)))\n","totalVal = len(list(paths.list_images(VAL_PATH)))\n","totalTest = len(list(paths.list_images(TEST_PATH)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ef9phyJPW571","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"33b4da4c-18d1-4fe9-a1c3-791407b91f63","executionInfo":{"status":"ok","timestamp":1554868219600,"user_tz":-420,"elapsed":1048,"user":{"displayName":"Võ Hoàng Tú","photoUrl":"","userId":"10850768615639726333"}}},"cell_type":"code","source":["# initialize the training training data augmentation object\n","trainAug = ImageDataGenerator(\n","\trescale=1 / 255.0,\n","\trotation_range=20,\n","\tzoom_range=0.05,\n","\twidth_shift_range=0.05,\n","\theight_shift_range=0.05,\n","\tshear_range=0.05,\n","\thorizontal_flip=True,\n","\tfill_mode=\"nearest\")\n"," \n","# initialize the validation (and testing) data augmentation object\n","valAug = ImageDataGenerator(rescale=1 / 255.0)\n","\n","# initialize the training generator\n","trainGen = trainAug.flow_from_directory(\n","\tTRAIN_PATH,\n","\tclass_mode=\"categorical\",\n","\ttarget_size=(64, 64),\n","\tcolor_mode=\"rgb\",\n","\tshuffle=True,\n","\tbatch_size=BS)\n"," \n","# initialize the validation generator\n","valGen = valAug.flow_from_directory(\n","\tVAL_PATH,\n","\tclass_mode=\"categorical\",\n","\ttarget_size=(64, 64),\n","\tcolor_mode=\"rgb\",\n","\tshuffle=False,\n","\tbatch_size=BS)\n"," \n","# initialize the testing generator\n","testGen = valAug.flow_from_directory(\n","\tTEST_PATH,\n","\tclass_mode=\"categorical\",\n","\ttarget_size=(64, 64),\n","\tcolor_mode=\"rgb\",\n","\tshuffle=False,\n","\tbatch_size=BS)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Found 360 images belonging to 2 classes.\n","Found 40 images belonging to 2 classes.\n","Found 100 images belonging to 2 classes.\n"],"name":"stdout"}]},{"metadata":{"id":"ZeY3WuYoYX8c","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":10861},"outputId":"f750d6d8-739d-44de-dd7b-092fea66162e","executionInfo":{"status":"ok","timestamp":1554870126857,"user_tz":-420,"elapsed":538630,"user":{"displayName":"Võ Hoàng Tú","photoUrl":"","userId":"10850768615639726333"}}},"cell_type":"code","source":["# initialize our Keras implementation of ResNet model and compile it\n","model = ResNet.build(64, 64, 3, 2, (2, 2, 3),\n","\t(32, 64, 128, 256), reg=0.0005)\n","opt = SGD(lr=1e-1, momentum=0.9, decay=1e-1 / NUM_EPOCHS)\n","model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n","\tmetrics=[\"accuracy\"])\n"," \n","# train our Keras model\n","H = model.fit_generator(\n","\ttrainGen,\n","\tsteps_per_epoch=totalTrain // BS,\n","\tvalidation_data=valGen,\n","\tvalidation_steps=totalVal // BS,\n","\tepochs=300)\n"," \n","# reset the testing generator and then use our trained model to\n","# make predictions on the data\n","print(\"[INFO] evaluating network...\")\n","testGen.reset()\n","predIdxs = model.predict_generator(testGen,\n","\tsteps=(totalTest // BS) + 1)\n"," \n","# for each image in the testing set we need to find the index of the\n","# label with corresponding largest predicted probability\n","predIdxs = np.argmax(predIdxs, axis=1)\n"," \n","# show a nicely formatted classification report\n","print(classification_report(testGen.classes, predIdxs,\n","\ttarget_names=testGen.class_indices.keys()))"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Epoch 1/300\n","11/11 [==============================] - 10s 876ms/step - loss: 1.0314 - acc: 0.5577 - val_loss: 2.9608 - val_acc: 0.4375\n","Epoch 2/300\n","11/11 [==============================] - 1s 74ms/step - loss: 1.3443 - acc: 0.4830 - val_loss: 16.1924 - val_acc: 0.0000e+00\n","Epoch 3/300\n","11/11 [==============================] - 1s 136ms/step - loss: 0.9704 - acc: 0.5387 - val_loss: 6.3937 - val_acc: 0.4375\n","Epoch 4/300\n","11/11 [==============================] - 2s 156ms/step - loss: 0.8692 - acc: 0.6042 - val_loss: 0.2117 - val_acc: 1.0000\n","Epoch 5/300\n","11/11 [==============================] - 2s 171ms/step - loss: 0.9463 - acc: 0.5379 - val_loss: 0.7365 - val_acc: 0.7188\n","Epoch 6/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.7544 - acc: 0.6843 - val_loss: 0.7002 - val_acc: 0.8750\n","Epoch 7/300\n","11/11 [==============================] - 2s 169ms/step - loss: 0.7157 - acc: 0.7329 - val_loss: 0.5558 - val_acc: 0.8125\n","Epoch 8/300\n","11/11 [==============================] - 2s 161ms/step - loss: 0.6078 - acc: 0.8056 - val_loss: 0.9526 - val_acc: 0.3750\n","Epoch 9/300\n","11/11 [==============================] - 2s 172ms/step - loss: 0.4966 - acc: 0.8828 - val_loss: 1.3274 - val_acc: 0.6250\n","Epoch 10/300\n","11/11 [==============================] - 2s 169ms/step - loss: 0.6556 - acc: 0.8337 - val_loss: 10.3335 - val_acc: 0.0000e+00\n","Epoch 11/300\n","11/11 [==============================] - 2s 170ms/step - loss: 0.6040 - acc: 0.8415 - val_loss: 0.6675 - val_acc: 0.8750\n","Epoch 12/300\n","11/11 [==============================] - 2s 154ms/step - loss: 0.5072 - acc: 0.8893 - val_loss: 0.8126 - val_acc: 0.7500\n","Epoch 13/300\n","11/11 [==============================] - 2s 162ms/step - loss: 0.4832 - acc: 0.8779 - val_loss: 0.6569 - val_acc: 0.9375\n","Epoch 14/300\n","11/11 [==============================] - 2s 165ms/step - loss: 0.4245 - acc: 0.9091 - val_loss: 0.5096 - val_acc: 1.0000\n","Epoch 15/300\n","11/11 [==============================] - 2s 164ms/step - loss: 0.4303 - acc: 0.9122 - val_loss: 0.3484 - val_acc: 0.9688\n","Epoch 16/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.4159 - acc: 0.9200 - val_loss: 0.2590 - val_acc: 1.0000\n","Epoch 17/300\n","11/11 [==============================] - 2s 171ms/step - loss: 0.3741 - acc: 0.9285 - val_loss: 0.2725 - val_acc: 1.0000\n","Epoch 18/300\n","11/11 [==============================] - 2s 167ms/step - loss: 0.3579 - acc: 0.9314 - val_loss: 0.2690 - val_acc: 1.0000\n","Epoch 19/300\n","11/11 [==============================] - 2s 169ms/step - loss: 0.4387 - acc: 0.9122 - val_loss: 0.2611 - val_acc: 0.9688\n","Epoch 20/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.3583 - acc: 0.9343 - val_loss: 0.3209 - val_acc: 1.0000\n","Epoch 21/300\n","11/11 [==============================] - 2s 176ms/step - loss: 0.3689 - acc: 0.9228 - val_loss: 0.2642 - val_acc: 0.9688\n","Epoch 22/300\n","11/11 [==============================] - 2s 162ms/step - loss: 0.3810 - acc: 0.9293 - val_loss: 0.4977 - val_acc: 0.8750\n","Epoch 23/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.3567 - acc: 0.9400 - val_loss: 0.3099 - val_acc: 0.9688\n","Epoch 24/300\n","11/11 [==============================] - 2s 162ms/step - loss: 0.3932 - acc: 0.9285 - val_loss: 0.2929 - val_acc: 1.0000\n","Epoch 25/300\n","11/11 [==============================] - 2s 157ms/step - loss: 0.3439 - acc: 0.9329 - val_loss: 0.3196 - val_acc: 0.9375\n","Epoch 26/300\n","11/11 [==============================] - 2s 165ms/step - loss: 0.3129 - acc: 0.9432 - val_loss: 0.4691 - val_acc: 0.8750\n","Epoch 27/300\n","11/11 [==============================] - 2s 162ms/step - loss: 0.3289 - acc: 0.9465 - val_loss: 0.2394 - val_acc: 0.9688\n","Epoch 28/300\n","11/11 [==============================] - 2s 164ms/step - loss: 0.2937 - acc: 0.9486 - val_loss: 0.4625 - val_acc: 0.8750\n","Epoch 29/300\n","11/11 [==============================] - 2s 175ms/step - loss: 0.3146 - acc: 0.9465 - val_loss: 0.3396 - val_acc: 0.8750\n","Epoch 30/300\n","11/11 [==============================] - 2s 195ms/step - loss: 0.2972 - acc: 0.9600 - val_loss: 0.4057 - val_acc: 0.8750\n","Epoch 31/300\n","11/11 [==============================] - 2s 184ms/step - loss: 0.3509 - acc: 0.9407 - val_loss: 0.3758 - val_acc: 0.9062\n","Epoch 32/300\n","11/11 [==============================] - 2s 179ms/step - loss: 0.3202 - acc: 0.9571 - val_loss: 0.3609 - val_acc: 0.8750\n","Epoch 33/300\n","11/11 [==============================] - 2s 171ms/step - loss: 0.3158 - acc: 0.9522 - val_loss: 0.2325 - val_acc: 1.0000\n","Epoch 34/300\n","11/11 [==============================] - 2s 167ms/step - loss: 0.2918 - acc: 0.9571 - val_loss: 0.5219 - val_acc: 0.8750\n","Epoch 35/300\n","11/11 [==============================] - 2s 160ms/step - loss: 0.2539 - acc: 0.9657 - val_loss: 0.2094 - val_acc: 1.0000\n","Epoch 36/300\n","11/11 [==============================] - 2s 162ms/step - loss: 0.3089 - acc: 0.9457 - val_loss: 0.4282 - val_acc: 0.8750\n","Epoch 37/300\n","11/11 [==============================] - 2s 172ms/step - loss: 0.2720 - acc: 0.9600 - val_loss: 0.2587 - val_acc: 0.9688\n","Epoch 38/300\n","11/11 [==============================] - 2s 169ms/step - loss: 0.2951 - acc: 0.9489 - val_loss: 0.5046 - val_acc: 0.8750\n","Epoch 39/300\n","11/11 [==============================] - 2s 174ms/step - loss: 0.2657 - acc: 0.9743 - val_loss: 0.2156 - val_acc: 0.9688\n","Epoch 40/300\n","11/11 [==============================] - 2s 169ms/step - loss: 0.2849 - acc: 0.9465 - val_loss: 1.1112 - val_acc: 0.8750\n","Epoch 41/300\n","11/11 [==============================] - 2s 168ms/step - loss: 0.3006 - acc: 0.9550 - val_loss: 0.6311 - val_acc: 0.9375\n","Epoch 42/300\n","11/11 [==============================] - 2s 154ms/step - loss: 0.2670 - acc: 0.9657 - val_loss: 0.9968 - val_acc: 0.8750\n","Epoch 43/300\n","11/11 [==============================] - 2s 173ms/step - loss: 0.2811 - acc: 0.9543 - val_loss: 0.2418 - val_acc: 1.0000\n","Epoch 44/300\n","11/11 [==============================] - 2s 172ms/step - loss: 0.2475 - acc: 0.9636 - val_loss: 0.7021 - val_acc: 0.8750\n","Epoch 45/300\n","11/11 [==============================] - 2s 173ms/step - loss: 0.2919 - acc: 0.9522 - val_loss: 0.2913 - val_acc: 0.9688\n","Epoch 46/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.3489 - acc: 0.9343 - val_loss: 0.4822 - val_acc: 0.8750\n","Epoch 47/300\n","11/11 [==============================] - 2s 160ms/step - loss: 0.3327 - acc: 0.9343 - val_loss: 0.4849 - val_acc: 0.8125\n","Epoch 48/300\n","11/11 [==============================] - 2s 170ms/step - loss: 0.3163 - acc: 0.9457 - val_loss: 0.4507 - val_acc: 0.8750\n","Epoch 49/300\n","11/11 [==============================] - 2s 165ms/step - loss: 0.2855 - acc: 0.9600 - val_loss: 0.2218 - val_acc: 1.0000\n","Epoch 50/300\n","11/11 [==============================] - 2s 174ms/step - loss: 0.3069 - acc: 0.9574 - val_loss: 0.9317 - val_acc: 0.8750\n","Epoch 51/300\n","11/11 [==============================] - 2s 168ms/step - loss: 0.2676 - acc: 0.9657 - val_loss: 0.2294 - val_acc: 1.0000\n","Epoch 52/300\n","11/11 [==============================] - 2s 167ms/step - loss: 0.2570 - acc: 0.9743 - val_loss: 1.1449 - val_acc: 0.8750\n","Epoch 53/300\n","11/11 [==============================] - 2s 172ms/step - loss: 0.2523 - acc: 0.9628 - val_loss: 0.1924 - val_acc: 1.0000\n","Epoch 54/300\n","11/11 [==============================] - 2s 167ms/step - loss: 0.2809 - acc: 0.9607 - val_loss: 0.7916 - val_acc: 0.8750\n","Epoch 55/300\n","11/11 [==============================] - 2s 168ms/step - loss: 0.3468 - acc: 0.9607 - val_loss: 0.2302 - val_acc: 0.9688\n","Epoch 56/300\n","11/11 [==============================] - 2s 165ms/step - loss: 0.3226 - acc: 0.9407 - val_loss: 0.5434 - val_acc: 0.8750\n","Epoch 57/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.2915 - acc: 0.9457 - val_loss: 0.2083 - val_acc: 1.0000\n","Epoch 58/300\n","11/11 [==============================] - 2s 155ms/step - loss: 0.4022 - acc: 0.9465 - val_loss: 0.9191 - val_acc: 0.8750\n","Epoch 59/300\n","11/11 [==============================] - 2s 170ms/step - loss: 0.2709 - acc: 0.9628 - val_loss: 0.2246 - val_acc: 1.0000\n","Epoch 60/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.2758 - acc: 0.9571 - val_loss: 0.6698 - val_acc: 0.8750\n","Epoch 61/300\n","11/11 [==============================] - 2s 164ms/step - loss: 0.3035 - acc: 0.9400 - val_loss: 0.2429 - val_acc: 0.9688\n","Epoch 62/300\n","11/11 [==============================] - 2s 168ms/step - loss: 0.2711 - acc: 0.9574 - val_loss: 0.6398 - val_acc: 0.8750\n","Epoch 63/300\n","11/11 [==============================] - 2s 152ms/step - loss: 0.2721 - acc: 0.9579 - val_loss: 0.2730 - val_acc: 0.9688\n","Epoch 64/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.2734 - acc: 0.9628 - val_loss: 0.8247 - val_acc: 0.8750\n","Epoch 65/300\n","11/11 [==============================] - 2s 166ms/step - loss: 0.2407 - acc: 0.9743 - val_loss: 0.1995 - val_acc: 1.0000\n","Epoch 66/300\n","11/11 [==============================] - 2s 157ms/step - loss: 0.2509 - acc: 0.9714 - val_loss: 1.2108 - val_acc: 0.8750\n","Epoch 67/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.2356 - acc: 0.9714 - val_loss: 0.2004 - val_acc: 1.0000\n","Epoch 68/300\n","11/11 [==============================] - 2s 152ms/step - loss: 0.2246 - acc: 0.9800 - val_loss: 1.4372 - val_acc: 0.8750\n","Epoch 69/300\n","11/11 [==============================] - 2s 155ms/step - loss: 0.3638 - acc: 0.9493 - val_loss: 2.5282 - val_acc: 0.5625\n","Epoch 70/300\n","11/11 [==============================] - 2s 150ms/step - loss: 0.3887 - acc: 0.9207 - val_loss: 0.2794 - val_acc: 1.0000\n","Epoch 71/300\n","11/11 [==============================] - 2s 160ms/step - loss: 0.3075 - acc: 0.9514 - val_loss: 0.2678 - val_acc: 0.9688\n","Epoch 72/300\n","11/11 [==============================] - 2s 146ms/step - loss: 0.2469 - acc: 0.9743 - val_loss: 0.3523 - val_acc: 0.8750\n","Epoch 73/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.2606 - acc: 0.9714 - val_loss: 0.2974 - val_acc: 0.9062\n","Epoch 74/300\n","11/11 [==============================] - 2s 161ms/step - loss: 0.2429 - acc: 0.9744 - val_loss: 0.4591 - val_acc: 0.8750\n","Epoch 75/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.2565 - acc: 0.9657 - val_loss: 0.2155 - val_acc: 1.0000\n","Epoch 76/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.2197 - acc: 0.9800 - val_loss: 0.4397 - val_acc: 0.8750\n","Epoch 77/300\n","11/11 [==============================] - 2s 164ms/step - loss: 0.2435 - acc: 0.9636 - val_loss: 0.2248 - val_acc: 1.0000\n","Epoch 78/300\n","11/11 [==============================] - 2s 161ms/step - loss: 0.2258 - acc: 0.9714 - val_loss: 0.7793 - val_acc: 0.8750\n","Epoch 79/300\n","11/11 [==============================] - 2s 169ms/step - loss: 0.2278 - acc: 0.9829 - val_loss: 0.2160 - val_acc: 0.9688\n","Epoch 80/300\n","11/11 [==============================] - 2s 166ms/step - loss: 0.2413 - acc: 0.9693 - val_loss: 0.7112 - val_acc: 0.8750\n","Epoch 81/300\n","11/11 [==============================] - 2s 173ms/step - loss: 0.2140 - acc: 0.9829 - val_loss: 0.1936 - val_acc: 1.0000\n","Epoch 82/300\n","11/11 [==============================] - 2s 166ms/step - loss: 0.2996 - acc: 0.9493 - val_loss: 0.4225 - val_acc: 0.8750\n","Epoch 83/300\n","11/11 [==============================] - 2s 171ms/step - loss: 0.2304 - acc: 0.9800 - val_loss: 0.2042 - val_acc: 1.0000\n","Epoch 84/300\n","11/11 [==============================] - 2s 161ms/step - loss: 0.2251 - acc: 0.9829 - val_loss: 1.2715 - val_acc: 0.8750\n","Epoch 85/300\n","11/11 [==============================] - 2s 168ms/step - loss: 0.2318 - acc: 0.9743 - val_loss: 0.1743 - val_acc: 1.0000\n","Epoch 86/300\n","11/11 [==============================] - 2s 175ms/step - loss: 0.2239 - acc: 0.9773 - val_loss: 0.9670 - val_acc: 0.8750\n","Epoch 87/300\n","11/11 [==============================] - 2s 167ms/step - loss: 0.2196 - acc: 0.9693 - val_loss: 0.1800 - val_acc: 1.0000\n","Epoch 88/300\n","11/11 [==============================] - 2s 160ms/step - loss: 0.2168 - acc: 0.9829 - val_loss: 1.1267 - val_acc: 0.8750\n","Epoch 89/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.2254 - acc: 0.9886 - val_loss: 0.1750 - val_acc: 1.0000\n","Epoch 90/300\n","11/11 [==============================] - 2s 162ms/step - loss: 0.1967 - acc: 0.9914 - val_loss: 1.3266 - val_acc: 0.8750\n","Epoch 91/300\n","11/11 [==============================] - 2s 169ms/step - loss: 0.2020 - acc: 0.9857 - val_loss: 0.1756 - val_acc: 1.0000\n","Epoch 92/300\n","11/11 [==============================] - 2s 159ms/step - loss: 0.2265 - acc: 0.9771 - val_loss: 1.0076 - val_acc: 0.8750\n","Epoch 93/300\n","11/11 [==============================] - 2s 162ms/step - loss: 0.2187 - acc: 0.9808 - val_loss: 0.2111 - val_acc: 1.0000\n","Epoch 94/300\n","11/11 [==============================] - 2s 164ms/step - loss: 0.2558 - acc: 0.9722 - val_loss: 1.5188 - val_acc: 0.8750\n","Epoch 95/300\n","11/11 [==============================] - 2s 165ms/step - loss: 0.2428 - acc: 0.9686 - val_loss: 0.2953 - val_acc: 0.9688\n","Epoch 96/300\n","11/11 [==============================] - 2s 147ms/step - loss: 0.2113 - acc: 0.9857 - val_loss: 1.1093 - val_acc: 0.8750\n","Epoch 97/300\n","11/11 [==============================] - 2s 161ms/step - loss: 0.2449 - acc: 0.9779 - val_loss: 0.1853 - val_acc: 1.0000\n","Epoch 98/300\n","11/11 [==============================] - 2s 172ms/step - loss: 0.2158 - acc: 0.9688 - val_loss: 0.6189 - val_acc: 0.8750\n","Epoch 99/300\n","11/11 [==============================] - 2s 164ms/step - loss: 0.2160 - acc: 0.9800 - val_loss: 0.1894 - val_acc: 1.0000\n","Epoch 100/300\n","11/11 [==============================] - 2s 161ms/step - loss: 0.2105 - acc: 0.9857 - val_loss: 0.7339 - val_acc: 0.8750\n","Epoch 101/300\n","11/11 [==============================] - 2s 171ms/step - loss: 0.2105 - acc: 0.9771 - val_loss: 0.2369 - val_acc: 0.9688\n","Epoch 102/300\n","11/11 [==============================] - 2s 160ms/step - loss: 0.2154 - acc: 0.9829 - val_loss: 1.2058 - val_acc: 0.8750\n","Epoch 103/300\n","11/11 [==============================] - 2s 162ms/step - loss: 0.2018 - acc: 0.9800 - val_loss: 0.1744 - val_acc: 1.0000\n","Epoch 104/300\n","11/11 [==============================] - 2s 154ms/step - loss: 0.1916 - acc: 0.9914 - val_loss: 1.2870 - val_acc: 0.8750\n","Epoch 105/300\n","11/11 [==============================] - 2s 156ms/step - loss: 0.1990 - acc: 0.9808 - val_loss: 0.1885 - val_acc: 1.0000\n","Epoch 106/300\n","11/11 [==============================] - 2s 164ms/step - loss: 0.2067 - acc: 0.9829 - val_loss: 1.5969 - val_acc: 0.8750\n","Epoch 107/300\n","11/11 [==============================] - 2s 169ms/step - loss: 0.1943 - acc: 0.9914 - val_loss: 0.1714 - val_acc: 1.0000\n","Epoch 108/300\n","11/11 [==============================] - 2s 165ms/step - loss: 0.6757 - acc: 0.9352 - val_loss: 6.7698 - val_acc: 0.0000e+00\n","Epoch 109/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.7001 - acc: 0.8131 - val_loss: 1.0037 - val_acc: 0.7500\n","Epoch 110/300\n","11/11 [==============================] - 2s 162ms/step - loss: 0.4963 - acc: 0.9119 - val_loss: 0.2604 - val_acc: 1.0000\n","Epoch 111/300\n","11/11 [==============================] - 2s 162ms/step - loss: 0.3128 - acc: 0.9543 - val_loss: 0.2901 - val_acc: 0.9688\n","Epoch 112/300\n","11/11 [==============================] - 2s 156ms/step - loss: 0.2850 - acc: 0.9550 - val_loss: 0.3779 - val_acc: 0.8750\n","Epoch 113/300\n","11/11 [==============================] - 2s 161ms/step - loss: 0.3907 - acc: 0.9322 - val_loss: 0.2685 - val_acc: 1.0000\n","Epoch 114/300\n","11/11 [==============================] - 2s 160ms/step - loss: 0.3204 - acc: 0.9358 - val_loss: 0.3212 - val_acc: 1.0000\n","Epoch 115/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.2660 - acc: 0.9600 - val_loss: 0.2516 - val_acc: 0.9688\n","Epoch 116/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.2556 - acc: 0.9607 - val_loss: 0.2375 - val_acc: 1.0000\n","Epoch 117/300\n","11/11 [==============================] - 2s 162ms/step - loss: 0.2730 - acc: 0.9600 - val_loss: 0.2210 - val_acc: 0.9688\n","Epoch 118/300\n","11/11 [==============================] - 2s 166ms/step - loss: 0.2637 - acc: 0.9657 - val_loss: 0.3254 - val_acc: 0.8750\n","Epoch 119/300\n","11/11 [==============================] - 2s 167ms/step - loss: 0.2853 - acc: 0.9493 - val_loss: 0.2020 - val_acc: 1.0000\n","Epoch 120/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.2665 - acc: 0.9600 - val_loss: 0.5008 - val_acc: 0.8750\n","Epoch 121/300\n","11/11 [==============================] - 2s 167ms/step - loss: 0.2448 - acc: 0.9771 - val_loss: 0.2014 - val_acc: 0.9688\n","Epoch 122/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.2564 - acc: 0.9631 - val_loss: 0.4274 - val_acc: 0.8750\n","Epoch 123/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.2420 - acc: 0.9686 - val_loss: 0.2058 - val_acc: 1.0000\n","Epoch 124/300\n","11/11 [==============================] - 2s 154ms/step - loss: 0.2263 - acc: 0.9686 - val_loss: 0.5106 - val_acc: 0.8750\n","Epoch 125/300\n","11/11 [==============================] - 2s 161ms/step - loss: 0.1909 - acc: 0.9943 - val_loss: 0.1898 - val_acc: 1.0000\n","Epoch 126/300\n","11/11 [==============================] - 2s 156ms/step - loss: 0.2506 - acc: 0.9628 - val_loss: 0.6025 - val_acc: 0.8750\n","Epoch 127/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.2129 - acc: 0.9829 - val_loss: 0.2128 - val_acc: 1.0000\n","Epoch 128/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.2354 - acc: 0.9771 - val_loss: 0.3288 - val_acc: 0.8750\n","Epoch 129/300\n","11/11 [==============================] - 2s 152ms/step - loss: 0.2343 - acc: 0.9714 - val_loss: 0.1895 - val_acc: 1.0000\n","Epoch 130/300\n","11/11 [==============================] - 2s 146ms/step - loss: 0.2214 - acc: 0.9771 - val_loss: 0.5378 - val_acc: 0.8750\n","Epoch 131/300\n","11/11 [==============================] - 2s 161ms/step - loss: 0.2018 - acc: 0.9829 - val_loss: 0.1737 - val_acc: 1.0000\n","Epoch 132/300\n","11/11 [==============================] - 2s 154ms/step - loss: 0.2500 - acc: 0.9607 - val_loss: 0.4286 - val_acc: 0.8750\n","Epoch 133/300\n","11/11 [==============================] - 2s 154ms/step - loss: 0.2401 - acc: 0.9579 - val_loss: 0.1850 - val_acc: 1.0000\n","Epoch 134/300\n","11/11 [==============================] - 2s 160ms/step - loss: 0.2247 - acc: 0.9716 - val_loss: 0.4261 - val_acc: 0.8750\n","Epoch 135/300\n","11/11 [==============================] - 2s 159ms/step - loss: 0.2211 - acc: 0.9771 - val_loss: 0.1958 - val_acc: 1.0000\n","Epoch 136/300\n","11/11 [==============================] - 2s 153ms/step - loss: 0.2269 - acc: 0.9686 - val_loss: 0.5034 - val_acc: 0.8750\n","Epoch 137/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.2031 - acc: 0.9800 - val_loss: 0.2009 - val_acc: 1.0000\n","Epoch 138/300\n","11/11 [==============================] - 2s 152ms/step - loss: 0.1979 - acc: 0.9943 - val_loss: 0.5155 - val_acc: 0.8750\n","Epoch 139/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.2031 - acc: 0.9857 - val_loss: 0.1737 - val_acc: 1.0000\n","Epoch 140/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.1985 - acc: 0.9800 - val_loss: 0.6254 - val_acc: 0.8750\n","Epoch 141/300\n","11/11 [==============================] - 2s 160ms/step - loss: 0.1994 - acc: 0.9800 - val_loss: 0.1751 - val_acc: 1.0000\n","Epoch 142/300\n","11/11 [==============================] - 2s 155ms/step - loss: 0.1958 - acc: 0.9857 - val_loss: 0.8534 - val_acc: 0.8750\n","Epoch 143/300\n","11/11 [==============================] - 2s 161ms/step - loss: 0.2434 - acc: 0.9701 - val_loss: 0.1667 - val_acc: 1.0000\n","Epoch 144/300\n","11/11 [==============================] - 2s 157ms/step - loss: 0.2335 - acc: 0.9686 - val_loss: 0.1681 - val_acc: 1.0000\n","Epoch 145/300\n","11/11 [==============================] - 2s 160ms/step - loss: 0.2179 - acc: 0.9714 - val_loss: 0.1807 - val_acc: 1.0000\n","Epoch 146/300\n","11/11 [==============================] - 2s 172ms/step - loss: 0.2054 - acc: 0.9801 - val_loss: 0.2093 - val_acc: 1.0000\n","Epoch 147/300\n","11/11 [==============================] - 2s 166ms/step - loss: 0.2435 - acc: 0.9636 - val_loss: 0.2343 - val_acc: 0.9688\n","Epoch 148/300\n","11/11 [==============================] - 2s 160ms/step - loss: 0.2836 - acc: 0.9607 - val_loss: 0.6543 - val_acc: 0.8750\n","Epoch 149/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.2434 - acc: 0.9600 - val_loss: 0.2279 - val_acc: 0.9688\n","Epoch 150/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.2128 - acc: 0.9771 - val_loss: 0.8817 - val_acc: 0.8750\n","Epoch 151/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.2559 - acc: 0.9693 - val_loss: 0.1970 - val_acc: 0.9688\n","Epoch 152/300\n","11/11 [==============================] - 2s 152ms/step - loss: 0.2106 - acc: 0.9829 - val_loss: 0.4173 - val_acc: 0.8750\n","Epoch 153/300\n","11/11 [==============================] - 2s 154ms/step - loss: 0.2198 - acc: 0.9722 - val_loss: 0.2197 - val_acc: 1.0000\n","Epoch 154/300\n","11/11 [==============================] - 2s 152ms/step - loss: 0.2459 - acc: 0.9615 - val_loss: 0.9755 - val_acc: 0.8750\n","Epoch 155/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.2218 - acc: 0.9771 - val_loss: 0.2451 - val_acc: 0.9688\n","Epoch 156/300\n","11/11 [==============================] - 2s 159ms/step - loss: 0.2069 - acc: 0.9800 - val_loss: 0.6303 - val_acc: 0.8750\n","Epoch 157/300\n","11/11 [==============================] - 2s 160ms/step - loss: 0.2122 - acc: 0.9615 - val_loss: 0.2175 - val_acc: 0.9688\n","Epoch 158/300\n","11/11 [==============================] - 2s 162ms/step - loss: 0.1981 - acc: 0.9858 - val_loss: 0.5082 - val_acc: 0.8750\n","Epoch 159/300\n","11/11 [==============================] - 2s 156ms/step - loss: 0.2034 - acc: 0.9857 - val_loss: 0.1853 - val_acc: 1.0000\n","Epoch 160/300\n","11/11 [==============================] - 2s 156ms/step - loss: 0.2148 - acc: 0.9829 - val_loss: 0.7611 - val_acc: 0.8750\n","Epoch 161/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.2005 - acc: 0.9857 - val_loss: 0.2234 - val_acc: 0.9688\n","Epoch 162/300\n","11/11 [==============================] - 2s 155ms/step - loss: 0.1950 - acc: 0.9829 - val_loss: 0.7214 - val_acc: 0.8750\n","Epoch 163/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.2512 - acc: 0.9636 - val_loss: 0.1828 - val_acc: 1.0000\n","Epoch 164/300\n","11/11 [==============================] - 2s 165ms/step - loss: 0.2013 - acc: 0.9886 - val_loss: 0.9046 - val_acc: 0.8750\n","Epoch 165/300\n","11/11 [==============================] - 2s 172ms/step - loss: 0.2193 - acc: 0.9693 - val_loss: 0.1735 - val_acc: 1.0000\n","Epoch 166/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.2129 - acc: 0.9857 - val_loss: 0.8899 - val_acc: 0.8750\n","Epoch 167/300\n","11/11 [==============================] - 2s 156ms/step - loss: 0.1941 - acc: 0.9857 - val_loss: 0.1721 - val_acc: 1.0000\n","Epoch 168/300\n","11/11 [==============================] - 2s 156ms/step - loss: 0.1862 - acc: 0.9943 - val_loss: 0.7381 - val_acc: 0.8750\n","Epoch 169/300\n","11/11 [==============================] - 2s 156ms/step - loss: 0.1845 - acc: 0.9943 - val_loss: 0.1647 - val_acc: 1.0000\n","Epoch 170/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.1929 - acc: 0.9830 - val_loss: 0.7676 - val_acc: 0.8750\n","Epoch 171/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.1784 - acc: 0.9943 - val_loss: 0.1644 - val_acc: 1.0000\n","Epoch 172/300\n","11/11 [==============================] - 2s 155ms/step - loss: 0.2201 - acc: 0.9750 - val_loss: 0.9651 - val_acc: 0.8750\n","Epoch 173/300\n","11/11 [==============================] - 2s 161ms/step - loss: 0.1792 - acc: 0.9943 - val_loss: 0.1672 - val_acc: 1.0000\n","Epoch 174/300\n","11/11 [==============================] - 2s 151ms/step - loss: 0.1800 - acc: 0.9943 - val_loss: 0.7155 - val_acc: 0.8750\n","Epoch 175/300\n","11/11 [==============================] - 2s 153ms/step - loss: 0.1868 - acc: 0.9886 - val_loss: 0.1725 - val_acc: 1.0000\n","Epoch 176/300\n","11/11 [==============================] - 2s 153ms/step - loss: 0.1862 - acc: 0.9914 - val_loss: 0.8367 - val_acc: 0.8750\n","Epoch 177/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.1975 - acc: 0.9779 - val_loss: 0.1618 - val_acc: 1.0000\n","Epoch 178/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.2058 - acc: 0.9829 - val_loss: 1.0617 - val_acc: 0.8750\n","Epoch 179/300\n","11/11 [==============================] - 2s 165ms/step - loss: 0.1704 - acc: 0.9943 - val_loss: 0.1685 - val_acc: 1.0000\n","Epoch 180/300\n","11/11 [==============================] - 2s 154ms/step - loss: 0.1899 - acc: 0.9914 - val_loss: 0.6684 - val_acc: 0.8750\n","Epoch 181/300\n","11/11 [==============================] - 2s 160ms/step - loss: 0.1718 - acc: 0.9943 - val_loss: 0.1862 - val_acc: 0.9688\n","Epoch 182/300\n","11/11 [==============================] - 2s 171ms/step - loss: 0.1827 - acc: 0.9886 - val_loss: 0.7204 - val_acc: 0.8750\n","Epoch 183/300\n","11/11 [==============================] - 2s 161ms/step - loss: 0.2119 - acc: 0.9857 - val_loss: 0.1895 - val_acc: 0.9688\n","Epoch 184/300\n","11/11 [==============================] - 2s 159ms/step - loss: 0.1744 - acc: 0.9943 - val_loss: 0.6860 - val_acc: 0.8750\n","Epoch 185/300\n","11/11 [==============================] - 2s 162ms/step - loss: 0.1963 - acc: 0.9886 - val_loss: 0.1887 - val_acc: 0.9688\n","Epoch 186/300\n","11/11 [==============================] - 2s 157ms/step - loss: 0.1739 - acc: 0.9943 - val_loss: 0.8840 - val_acc: 0.8750\n","Epoch 187/300\n","11/11 [==============================] - 2s 154ms/step - loss: 0.1691 - acc: 0.9943 - val_loss: 0.1950 - val_acc: 0.9688\n","Epoch 188/300\n","11/11 [==============================] - 2s 154ms/step - loss: 0.1844 - acc: 0.9886 - val_loss: 0.8016 - val_acc: 0.8750\n","Epoch 189/300\n","11/11 [==============================] - 2s 160ms/step - loss: 0.1713 - acc: 0.9943 - val_loss: 0.2107 - val_acc: 0.9688\n","Epoch 190/300\n","11/11 [==============================] - 2s 153ms/step - loss: 0.1793 - acc: 0.9914 - val_loss: 0.7783 - val_acc: 0.8750\n","Epoch 191/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.1794 - acc: 0.9914 - val_loss: 0.1713 - val_acc: 1.0000\n","Epoch 192/300\n","11/11 [==============================] - 2s 154ms/step - loss: 0.2266 - acc: 0.9771 - val_loss: 0.6185 - val_acc: 0.8750\n","Epoch 193/300\n","11/11 [==============================] - 2s 159ms/step - loss: 0.2014 - acc: 0.9829 - val_loss: 0.1883 - val_acc: 0.9688\n","Epoch 194/300\n","11/11 [==============================] - 2s 161ms/step - loss: 0.1889 - acc: 0.9858 - val_loss: 1.1175 - val_acc: 0.8750\n","Epoch 195/300\n","11/11 [==============================] - 2s 157ms/step - loss: 0.2376 - acc: 0.9693 - val_loss: 0.1763 - val_acc: 1.0000\n","Epoch 196/300\n","11/11 [==============================] - 2s 154ms/step - loss: 0.2109 - acc: 0.9771 - val_loss: 1.2910 - val_acc: 0.8750\n","Epoch 197/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.1817 - acc: 0.9914 - val_loss: 0.1584 - val_acc: 1.0000\n","Epoch 198/300\n","11/11 [==============================] - 2s 150ms/step - loss: 0.1691 - acc: 0.9971 - val_loss: 1.2893 - val_acc: 0.8750\n","Epoch 199/300\n","11/11 [==============================] - 2s 153ms/step - loss: 0.1745 - acc: 0.9914 - val_loss: 0.1597 - val_acc: 1.0000\n","Epoch 200/300\n","11/11 [==============================] - 2s 146ms/step - loss: 0.1859 - acc: 0.9886 - val_loss: 0.9351 - val_acc: 0.8750\n","Epoch 201/300\n","11/11 [==============================] - 2s 167ms/step - loss: 0.1919 - acc: 0.9808 - val_loss: 0.1582 - val_acc: 1.0000\n","Epoch 202/300\n","11/11 [==============================] - 2s 171ms/step - loss: 0.1695 - acc: 0.9914 - val_loss: 1.1449 - val_acc: 0.8750\n","Epoch 203/300\n","11/11 [==============================] - 2s 185ms/step - loss: 0.1772 - acc: 0.9914 - val_loss: 0.1581 - val_acc: 1.0000\n","Epoch 204/300\n","11/11 [==============================] - 2s 166ms/step - loss: 0.2034 - acc: 0.9829 - val_loss: 0.8536 - val_acc: 0.8750\n","Epoch 205/300\n","11/11 [==============================] - 2s 166ms/step - loss: 0.1854 - acc: 0.9943 - val_loss: 0.1641 - val_acc: 1.0000\n","Epoch 206/300\n","11/11 [==============================] - 2s 170ms/step - loss: 0.1834 - acc: 0.9801 - val_loss: 0.9558 - val_acc: 0.8750\n","Epoch 207/300\n","11/11 [==============================] - 2s 159ms/step - loss: 0.1731 - acc: 0.9914 - val_loss: 0.1862 - val_acc: 0.9688\n","Epoch 208/300\n","11/11 [==============================] - 2s 153ms/step - loss: 0.1686 - acc: 0.9943 - val_loss: 0.9581 - val_acc: 0.8750\n","Epoch 209/300\n","11/11 [==============================] - 2s 158ms/step - loss: 0.1741 - acc: 0.9971 - val_loss: 0.2248 - val_acc: 0.9688\n","Epoch 210/300\n","11/11 [==============================] - 2s 155ms/step - loss: 0.1699 - acc: 0.9943 - val_loss: 0.9745 - val_acc: 0.8750\n","Epoch 211/300\n","11/11 [==============================] - 2s 166ms/step - loss: 0.1816 - acc: 0.9886 - val_loss: 0.2136 - val_acc: 0.9688\n","Epoch 212/300\n","11/11 [==============================] - 2s 162ms/step - loss: 0.1667 - acc: 0.9971 - val_loss: 1.0186 - val_acc: 0.8750\n","Epoch 213/300\n","11/11 [==============================] - 2s 154ms/step - loss: 0.1706 - acc: 0.9943 - val_loss: 0.2038 - val_acc: 0.9688\n","Epoch 214/300\n","11/11 [==============================] - 2s 165ms/step - loss: 0.1843 - acc: 0.9886 - val_loss: 0.8739 - val_acc: 0.8750\n","Epoch 215/300\n","11/11 [==============================] - 2s 169ms/step - loss: 0.2500 - acc: 0.9750 - val_loss: 0.2775 - val_acc: 0.9688\n","Epoch 216/300\n","11/11 [==============================] - 2s 157ms/step - loss: 0.1971 - acc: 0.9800 - val_loss: 1.0893 - val_acc: 0.8750\n","Epoch 217/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.1999 - acc: 0.9800 - val_loss: 0.2736 - val_acc: 0.9688\n","Epoch 218/300\n","11/11 [==============================] - 2s 174ms/step - loss: 0.1888 - acc: 0.9858 - val_loss: 0.9551 - val_acc: 0.8750\n","Epoch 219/300\n","11/11 [==============================] - 2s 169ms/step - loss: 0.1732 - acc: 0.9943 - val_loss: 0.2362 - val_acc: 0.9688\n","Epoch 220/300\n","11/11 [==============================] - 2s 166ms/step - loss: 0.1969 - acc: 0.9779 - val_loss: 1.2185 - val_acc: 0.8750\n","Epoch 221/300\n","11/11 [==============================] - 2s 163ms/step - loss: 0.1800 - acc: 0.9886 - val_loss: 0.2462 - val_acc: 0.9688\n","Epoch 222/300\n","11/11 [==============================] - 2s 159ms/step - loss: 0.1684 - acc: 0.9914 - val_loss: 0.7230 - val_acc: 0.8750\n","Epoch 223/300\n","11/11 [==============================] - 2s 156ms/step - loss: 0.1702 - acc: 1.0000 - val_loss: 0.2666 - val_acc: 0.9688\n","Epoch 224/300\n","11/11 [==============================] - 2s 143ms/step - loss: 0.1823 - acc: 0.9886 - val_loss: 0.5773 - val_acc: 0.8750\n","Epoch 225/300\n","11/11 [==============================] - 2s 147ms/step - loss: 0.1818 - acc: 0.9886 - val_loss: 0.2145 - val_acc: 0.9688\n","Epoch 226/300\n","11/11 [==============================] - 2s 144ms/step - loss: 0.2153 - acc: 0.9808 - val_loss: 0.5263 - val_acc: 0.8750\n","Epoch 227/300\n","11/11 [==============================] - 2s 155ms/step - loss: 0.1709 - acc: 0.9943 - val_loss: 0.2404 - val_acc: 0.9688\n","Epoch 228/300\n","11/11 [==============================] - 2s 152ms/step - loss: 0.1758 - acc: 0.9943 - val_loss: 0.7842 - val_acc: 0.8750\n","Epoch 229/300\n","11/11 [==============================] - 2s 149ms/step - loss: 0.1806 - acc: 0.9865 - val_loss: 0.2454 - val_acc: 0.9688\n","Epoch 230/300\n","11/11 [==============================] - 2s 152ms/step - loss: 0.1646 - acc: 0.9972 - val_loss: 0.6889 - val_acc: 0.8750\n","Epoch 231/300\n","11/11 [==============================] - 2s 145ms/step - loss: 0.1674 - acc: 0.9971 - val_loss: 0.1605 - val_acc: 1.0000\n","Epoch 232/300\n","11/11 [==============================] - 2s 144ms/step - loss: 0.1637 - acc: 0.9943 - val_loss: 0.9604 - val_acc: 0.8750\n","Epoch 233/300\n","11/11 [==============================] - 2s 146ms/step - loss: 0.1710 - acc: 0.9914 - val_loss: 0.1647 - val_acc: 1.0000\n","Epoch 234/300\n","11/11 [==============================] - 2s 141ms/step - loss: 0.1637 - acc: 1.0000 - val_loss: 0.8432 - val_acc: 0.8750\n","Epoch 235/300\n","11/11 [==============================] - 2s 146ms/step - loss: 0.1812 - acc: 0.9886 - val_loss: 0.2427 - val_acc: 0.9688\n","Epoch 236/300\n","11/11 [==============================] - 2s 145ms/step - loss: 0.1677 - acc: 0.9914 - val_loss: 0.5425 - val_acc: 0.8750\n","Epoch 237/300\n","11/11 [==============================] - 2s 144ms/step - loss: 0.1712 - acc: 0.9971 - val_loss: 0.1855 - val_acc: 0.9688\n","Epoch 238/300\n","11/11 [==============================] - 2s 143ms/step - loss: 0.1707 - acc: 0.9914 - val_loss: 0.7211 - val_acc: 0.8750\n","Epoch 239/300\n","11/11 [==============================] - 2s 139ms/step - loss: 0.1731 - acc: 0.9943 - val_loss: 0.1616 - val_acc: 1.0000\n","Epoch 240/300\n","11/11 [==============================] - 1s 135ms/step - loss: 0.1539 - acc: 1.0000 - val_loss: 0.6558 - val_acc: 0.8750\n","Epoch 241/300\n","11/11 [==============================] - 2s 138ms/step - loss: 0.2165 - acc: 0.9779 - val_loss: 0.2545 - val_acc: 0.9688\n","Epoch 242/300\n","11/11 [==============================] - 2s 148ms/step - loss: 0.1643 - acc: 0.9943 - val_loss: 0.6350 - val_acc: 0.8750\n","Epoch 243/300\n","11/11 [==============================] - 2s 144ms/step - loss: 0.1843 - acc: 0.9914 - val_loss: 0.2100 - val_acc: 0.9688\n","Epoch 244/300\n","11/11 [==============================] - 2s 138ms/step - loss: 0.1638 - acc: 0.9971 - val_loss: 0.8384 - val_acc: 0.8750\n","Epoch 245/300\n","11/11 [==============================] - 2s 147ms/step - loss: 0.1688 - acc: 0.9886 - val_loss: 0.1615 - val_acc: 1.0000\n","Epoch 246/300\n","11/11 [==============================] - 2s 145ms/step - loss: 0.1837 - acc: 0.9893 - val_loss: 0.8151 - val_acc: 0.8750\n","Epoch 247/300\n","11/11 [==============================] - 2s 144ms/step - loss: 0.1630 - acc: 0.9971 - val_loss: 0.1545 - val_acc: 1.0000\n","Epoch 248/300\n","11/11 [==============================] - 2s 144ms/step - loss: 0.1890 - acc: 0.9865 - val_loss: 0.4527 - val_acc: 0.8750\n","Epoch 249/300\n","11/11 [==============================] - 2s 147ms/step - loss: 0.1686 - acc: 1.0000 - val_loss: 0.1648 - val_acc: 1.0000\n","Epoch 250/300\n","11/11 [==============================] - 2s 145ms/step - loss: 0.1601 - acc: 1.0000 - val_loss: 0.8350 - val_acc: 0.8750\n","Epoch 251/300\n","11/11 [==============================] - 2s 144ms/step - loss: 0.1625 - acc: 0.9971 - val_loss: 0.2532 - val_acc: 0.9688\n","Epoch 252/300\n","11/11 [==============================] - 2s 142ms/step - loss: 0.1674 - acc: 0.9943 - val_loss: 0.6509 - val_acc: 0.8750\n","Epoch 253/300\n","11/11 [==============================] - 2s 148ms/step - loss: 0.2594 - acc: 0.9808 - val_loss: 0.2683 - val_acc: 0.9688\n","Epoch 254/300\n","11/11 [==============================] - 2s 148ms/step - loss: 0.1561 - acc: 1.0000 - val_loss: 0.6230 - val_acc: 0.8750\n","Epoch 255/300\n","11/11 [==============================] - 2s 147ms/step - loss: 0.1636 - acc: 0.9943 - val_loss: 0.1610 - val_acc: 1.0000\n","Epoch 256/300\n","11/11 [==============================] - 2s 142ms/step - loss: 0.1672 - acc: 0.9914 - val_loss: 0.9431 - val_acc: 0.8750\n","Epoch 257/300\n","11/11 [==============================] - 2s 144ms/step - loss: 0.2266 - acc: 0.9779 - val_loss: 0.1939 - val_acc: 0.9688\n","Epoch 258/300\n","11/11 [==============================] - 2s 142ms/step - loss: 0.1606 - acc: 0.9971 - val_loss: 0.6574 - val_acc: 0.8750\n","Epoch 259/300\n","11/11 [==============================] - 2s 146ms/step - loss: 0.2115 - acc: 0.9750 - val_loss: 0.2234 - val_acc: 0.9688\n","Epoch 260/300\n","11/11 [==============================] - 2s 143ms/step - loss: 0.1996 - acc: 0.9800 - val_loss: 0.3781 - val_acc: 0.8750\n","Epoch 261/300\n","11/11 [==============================] - 2s 147ms/step - loss: 0.2116 - acc: 0.9829 - val_loss: 0.3093 - val_acc: 0.9688\n","Epoch 262/300\n","11/11 [==============================] - 2s 140ms/step - loss: 0.2030 - acc: 0.9771 - val_loss: 0.6382 - val_acc: 0.8750\n","Epoch 263/300\n","11/11 [==============================] - 2s 145ms/step - loss: 0.1781 - acc: 0.9886 - val_loss: 0.1877 - val_acc: 0.9688\n","Epoch 264/300\n","11/11 [==============================] - 2s 142ms/step - loss: 0.1680 - acc: 0.9914 - val_loss: 0.8815 - val_acc: 0.8750\n","Epoch 265/300\n","11/11 [==============================] - 2s 146ms/step - loss: 0.1600 - acc: 1.0000 - val_loss: 0.1729 - val_acc: 0.9688\n","Epoch 266/300\n","11/11 [==============================] - 2s 152ms/step - loss: 0.1822 - acc: 0.9915 - val_loss: 0.5992 - val_acc: 0.8750\n","Epoch 267/300\n","11/11 [==============================] - 2s 153ms/step - loss: 0.1682 - acc: 0.9914 - val_loss: 0.1875 - val_acc: 0.9688\n","Epoch 268/300\n","11/11 [==============================] - 2s 177ms/step - loss: 0.1610 - acc: 0.9971 - val_loss: 0.7666 - val_acc: 0.8750\n","Epoch 269/300\n","11/11 [==============================] - 2s 170ms/step - loss: 0.2390 - acc: 0.9787 - val_loss: 0.1659 - val_acc: 1.0000\n","Epoch 270/300\n","11/11 [==============================] - 2s 155ms/step - loss: 0.1946 - acc: 0.9886 - val_loss: 1.0754 - val_acc: 0.8750\n","Epoch 271/300\n","11/11 [==============================] - 2s 151ms/step - loss: 0.1661 - acc: 1.0000 - val_loss: 0.1546 - val_acc: 1.0000\n","Epoch 272/300\n","11/11 [==============================] - 2s 147ms/step - loss: 0.1653 - acc: 0.9943 - val_loss: 1.1196 - val_acc: 0.8750\n","Epoch 273/300\n","11/11 [==============================] - 2s 151ms/step - loss: 0.1680 - acc: 0.9971 - val_loss: 0.1887 - val_acc: 0.9688\n","Epoch 274/300\n","11/11 [==============================] - 2s 145ms/step - loss: 0.1653 - acc: 0.9914 - val_loss: 0.6569 - val_acc: 0.8750\n","Epoch 275/300\n","11/11 [==============================] - 2s 151ms/step - loss: 0.1643 - acc: 0.9943 - val_loss: 0.2754 - val_acc: 0.9688\n","Epoch 276/300\n","11/11 [==============================] - 2s 146ms/step - loss: 0.1669 - acc: 0.9943 - val_loss: 0.7288 - val_acc: 0.8750\n","Epoch 277/300\n","11/11 [==============================] - 2s 151ms/step - loss: 0.1546 - acc: 0.9971 - val_loss: 0.2174 - val_acc: 0.9688\n","Epoch 278/300\n","11/11 [==============================] - 2s 151ms/step - loss: 0.1605 - acc: 0.9972 - val_loss: 0.9353 - val_acc: 0.8750\n","Epoch 279/300\n","11/11 [==============================] - 2s 148ms/step - loss: 0.1652 - acc: 0.9943 - val_loss: 0.1914 - val_acc: 0.9688\n","Epoch 280/300\n","11/11 [==============================] - 2s 144ms/step - loss: 0.1708 - acc: 0.9943 - val_loss: 0.9155 - val_acc: 0.8750\n","Epoch 281/300\n","11/11 [==============================] - 2s 151ms/step - loss: 0.1609 - acc: 0.9971 - val_loss: 0.2535 - val_acc: 0.9688\n","Epoch 282/300\n","11/11 [==============================] - 2s 144ms/step - loss: 0.1692 - acc: 0.9943 - val_loss: 0.6273 - val_acc: 0.8750\n","Epoch 283/300\n","11/11 [==============================] - 2s 148ms/step - loss: 0.1544 - acc: 1.0000 - val_loss: 0.3085 - val_acc: 0.9688\n","Epoch 284/300\n","11/11 [==============================] - 2s 145ms/step - loss: 0.1569 - acc: 0.9943 - val_loss: 0.5546 - val_acc: 0.8750\n","Epoch 285/300\n","11/11 [==============================] - 2s 147ms/step - loss: 0.1611 - acc: 0.9943 - val_loss: 0.2231 - val_acc: 0.9688\n","Epoch 286/300\n","11/11 [==============================] - 2s 143ms/step - loss: 0.1709 - acc: 0.9943 - val_loss: 0.6426 - val_acc: 0.8750\n","Epoch 287/300\n","11/11 [==============================] - 2s 147ms/step - loss: 0.1673 - acc: 0.9886 - val_loss: 0.2953 - val_acc: 0.9688\n","Epoch 288/300\n","11/11 [==============================] - 2s 145ms/step - loss: 0.1800 - acc: 0.9857 - val_loss: 0.8504 - val_acc: 0.8750\n","Epoch 289/300\n","11/11 [==============================] - 2s 145ms/step - loss: 0.1535 - acc: 1.0000 - val_loss: 0.3321 - val_acc: 0.9688\n","Epoch 290/300\n","11/11 [==============================] - 2s 152ms/step - loss: 0.1577 - acc: 0.9972 - val_loss: 0.6526 - val_acc: 0.8750\n","Epoch 291/300\n","11/11 [==============================] - 2s 146ms/step - loss: 0.1727 - acc: 0.9914 - val_loss: 0.2039 - val_acc: 0.9688\n","Epoch 292/300\n","11/11 [==============================] - 2s 144ms/step - loss: 0.4782 - acc: 0.9280 - val_loss: 0.7735 - val_acc: 0.8750\n","Epoch 293/300\n","11/11 [==============================] - 2s 156ms/step - loss: 0.3681 - acc: 0.9379 - val_loss: 0.2632 - val_acc: 0.9688\n","Epoch 294/300\n","11/11 [==============================] - 2s 154ms/step - loss: 0.2149 - acc: 0.9800 - val_loss: 1.4188 - val_acc: 0.8750\n","Epoch 295/300\n","11/11 [==============================] - 2s 161ms/step - loss: 0.1989 - acc: 0.9779 - val_loss: 0.1549 - val_acc: 1.0000\n","Epoch 296/300\n","11/11 [==============================] - 2s 149ms/step - loss: 0.1753 - acc: 0.9943 - val_loss: 0.7903 - val_acc: 0.8750\n","Epoch 297/300\n","11/11 [==============================] - 2s 149ms/step - loss: 0.2123 - acc: 0.9750 - val_loss: 0.1651 - val_acc: 1.0000\n","Epoch 298/300\n","11/11 [==============================] - 2s 150ms/step - loss: 0.1798 - acc: 0.9914 - val_loss: 1.0214 - val_acc: 0.8750\n","Epoch 299/300\n","11/11 [==============================] - 2s 149ms/step - loss: 0.1700 - acc: 0.9914 - val_loss: 0.1558 - val_acc: 1.0000\n","Epoch 300/300\n","11/11 [==============================] - 2s 144ms/step - loss: 0.1776 - acc: 0.9857 - val_loss: 0.9563 - val_acc: 0.8750\n","[INFO] evaluating network...\n","              precision    recall  f1-score   support\n","\n"," Parasitized       1.00      0.98      0.99        50\n","  Uninfected       0.98      1.00      0.99        50\n","\n","   micro avg       0.99      0.99      0.99       100\n","   macro avg       0.99      0.99      0.99       100\n","weighted avg       0.99      0.99      0.99       100\n","\n"],"name":"stdout"}]},{"metadata":{"id":"5qgITP-zXjnW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ff9ad241-7a56-49e5-a6b3-fc0e3421b666","executionInfo":{"status":"ok","timestamp":1554868686098,"user_tz":-420,"elapsed":11996,"user":{"displayName":"Võ Hoàng Tú","photoUrl":"","userId":"10850768615639726333"}}},"cell_type":"code","source":["# save the network to disk\n","print(\"[INFO] serializing network to '{}'...\".format(\"/content/gdrive/My Drive/app/malaria-SotXuatHuyet/modelResnet.model\"))\n","model.save(\"/content/gdrive/My Drive/app/malaria-SotXuatHuyet/modelResnet.model\")"],"execution_count":25,"outputs":[{"output_type":"stream","text":["[INFO] serializing network to '/content/gdrive/My Drive/app/malaria-SotXuatHuyet/modelResnet.model'...\n"],"name":"stdout"}]},{"metadata":{"id":"igdySVn5X7J1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":3323},"outputId":"429551ca-ba7d-49b9-9444-dd8974ce2dcd","executionInfo":{"status":"ok","timestamp":1554875266517,"user_tz":-420,"elapsed":1185,"user":{"displayName":"Võ Hoàng Tú","photoUrl":"","userId":"10850768615639726333"}}},"cell_type":"code","source":["model.summary()"],"execution_count":27,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            (None, 64, 64, 3)    0                                            \n","__________________________________________________________________________________________________\n","batch_normalization_49 (BatchNo (None, 64, 64, 3)    12          input_3[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_51 (Conv2D)              (None, 64, 64, 32)   2400        batch_normalization_49[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_50 (BatchNo (None, 64, 64, 32)   128         conv2d_51[0][0]                  \n","__________________________________________________________________________________________________\n","activation_49 (Activation)      (None, 64, 64, 32)   0           batch_normalization_50[0][0]     \n","__________________________________________________________________________________________________\n","zero_padding2d_3 (ZeroPadding2D (None, 66, 66, 32)   0           activation_49[0][0]              \n","__________________________________________________________________________________________________\n","max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 32)   0           zero_padding2d_3[0][0]           \n","__________________________________________________________________________________________________\n","batch_normalization_51 (BatchNo (None, 32, 32, 32)   128         max_pooling2d_3[0][0]            \n","__________________________________________________________________________________________________\n","activation_50 (Activation)      (None, 32, 32, 32)   0           batch_normalization_51[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_52 (Conv2D)              (None, 32, 32, 16)   512         activation_50[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_52 (BatchNo (None, 32, 32, 16)   64          conv2d_52[0][0]                  \n","__________________________________________________________________________________________________\n","activation_51 (Activation)      (None, 32, 32, 16)   0           batch_normalization_52[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_53 (Conv2D)              (None, 32, 32, 16)   2304        activation_51[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_53 (BatchNo (None, 32, 32, 16)   64          conv2d_53[0][0]                  \n","__________________________________________________________________________________________________\n","activation_52 (Activation)      (None, 32, 32, 16)   0           batch_normalization_53[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_54 (Conv2D)              (None, 32, 32, 64)   1024        activation_52[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_55 (Conv2D)              (None, 32, 32, 64)   2048        activation_50[0][0]              \n","__________________________________________________________________________________________________\n","add_15 (Add)                    (None, 32, 32, 64)   0           conv2d_54[0][0]                  \n","                                                                 conv2d_55[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_54 (BatchNo (None, 32, 32, 64)   256         add_15[0][0]                     \n","__________________________________________________________________________________________________\n","activation_53 (Activation)      (None, 32, 32, 64)   0           batch_normalization_54[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_56 (Conv2D)              (None, 32, 32, 16)   1024        activation_53[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_55 (BatchNo (None, 32, 32, 16)   64          conv2d_56[0][0]                  \n","__________________________________________________________________________________________________\n","activation_54 (Activation)      (None, 32, 32, 16)   0           batch_normalization_55[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_57 (Conv2D)              (None, 32, 32, 16)   2304        activation_54[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_56 (BatchNo (None, 32, 32, 16)   64          conv2d_57[0][0]                  \n","__________________________________________________________________________________________________\n","activation_55 (Activation)      (None, 32, 32, 16)   0           batch_normalization_56[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_58 (Conv2D)              (None, 32, 32, 64)   1024        activation_55[0][0]              \n","__________________________________________________________________________________________________\n","add_16 (Add)                    (None, 32, 32, 64)   0           conv2d_58[0][0]                  \n","                                                                 add_15[0][0]                     \n","__________________________________________________________________________________________________\n","batch_normalization_57 (BatchNo (None, 32, 32, 64)   256         add_16[0][0]                     \n","__________________________________________________________________________________________________\n","activation_56 (Activation)      (None, 32, 32, 64)   0           batch_normalization_57[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_59 (Conv2D)              (None, 32, 32, 32)   2048        activation_56[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_58 (BatchNo (None, 32, 32, 32)   128         conv2d_59[0][0]                  \n","__________________________________________________________________________________________________\n","activation_57 (Activation)      (None, 32, 32, 32)   0           batch_normalization_58[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_60 (Conv2D)              (None, 16, 16, 32)   9216        activation_57[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_59 (BatchNo (None, 16, 16, 32)   128         conv2d_60[0][0]                  \n","__________________________________________________________________________________________________\n","activation_58 (Activation)      (None, 16, 16, 32)   0           batch_normalization_59[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_61 (Conv2D)              (None, 16, 16, 128)  4096        activation_58[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_62 (Conv2D)              (None, 16, 16, 128)  8192        activation_56[0][0]              \n","__________________________________________________________________________________________________\n","add_17 (Add)                    (None, 16, 16, 128)  0           conv2d_61[0][0]                  \n","                                                                 conv2d_62[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_60 (BatchNo (None, 16, 16, 128)  512         add_17[0][0]                     \n","__________________________________________________________________________________________________\n","activation_59 (Activation)      (None, 16, 16, 128)  0           batch_normalization_60[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_63 (Conv2D)              (None, 16, 16, 32)   4096        activation_59[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_61 (BatchNo (None, 16, 16, 32)   128         conv2d_63[0][0]                  \n","__________________________________________________________________________________________________\n","activation_60 (Activation)      (None, 16, 16, 32)   0           batch_normalization_61[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_64 (Conv2D)              (None, 16, 16, 32)   9216        activation_60[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_62 (BatchNo (None, 16, 16, 32)   128         conv2d_64[0][0]                  \n","__________________________________________________________________________________________________\n","activation_61 (Activation)      (None, 16, 16, 32)   0           batch_normalization_62[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_65 (Conv2D)              (None, 16, 16, 128)  4096        activation_61[0][0]              \n","__________________________________________________________________________________________________\n","add_18 (Add)                    (None, 16, 16, 128)  0           conv2d_65[0][0]                  \n","                                                                 add_17[0][0]                     \n","__________________________________________________________________________________________________\n","batch_normalization_63 (BatchNo (None, 16, 16, 128)  512         add_18[0][0]                     \n","__________________________________________________________________________________________________\n","activation_62 (Activation)      (None, 16, 16, 128)  0           batch_normalization_63[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_66 (Conv2D)              (None, 16, 16, 64)   8192        activation_62[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_64 (BatchNo (None, 16, 16, 64)   256         conv2d_66[0][0]                  \n","__________________________________________________________________________________________________\n","activation_63 (Activation)      (None, 16, 16, 64)   0           batch_normalization_64[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_67 (Conv2D)              (None, 8, 8, 64)     36864       activation_63[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_65 (BatchNo (None, 8, 8, 64)     256         conv2d_67[0][0]                  \n","__________________________________________________________________________________________________\n","activation_64 (Activation)      (None, 8, 8, 64)     0           batch_normalization_65[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_68 (Conv2D)              (None, 8, 8, 256)    16384       activation_64[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_69 (Conv2D)              (None, 8, 8, 256)    32768       activation_62[0][0]              \n","__________________________________________________________________________________________________\n","add_19 (Add)                    (None, 8, 8, 256)    0           conv2d_68[0][0]                  \n","                                                                 conv2d_69[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_66 (BatchNo (None, 8, 8, 256)    1024        add_19[0][0]                     \n","__________________________________________________________________________________________________\n","activation_65 (Activation)      (None, 8, 8, 256)    0           batch_normalization_66[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_70 (Conv2D)              (None, 8, 8, 64)     16384       activation_65[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_67 (BatchNo (None, 8, 8, 64)     256         conv2d_70[0][0]                  \n","__________________________________________________________________________________________________\n","activation_66 (Activation)      (None, 8, 8, 64)     0           batch_normalization_67[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_71 (Conv2D)              (None, 8, 8, 64)     36864       activation_66[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_68 (BatchNo (None, 8, 8, 64)     256         conv2d_71[0][0]                  \n","__________________________________________________________________________________________________\n","activation_67 (Activation)      (None, 8, 8, 64)     0           batch_normalization_68[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_72 (Conv2D)              (None, 8, 8, 256)    16384       activation_67[0][0]              \n","__________________________________________________________________________________________________\n","add_20 (Add)                    (None, 8, 8, 256)    0           conv2d_72[0][0]                  \n","                                                                 add_19[0][0]                     \n","__________________________________________________________________________________________________\n","batch_normalization_69 (BatchNo (None, 8, 8, 256)    1024        add_20[0][0]                     \n","__________________________________________________________________________________________________\n","activation_68 (Activation)      (None, 8, 8, 256)    0           batch_normalization_69[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_73 (Conv2D)              (None, 8, 8, 64)     16384       activation_68[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_70 (BatchNo (None, 8, 8, 64)     256         conv2d_73[0][0]                  \n","__________________________________________________________________________________________________\n","activation_69 (Activation)      (None, 8, 8, 64)     0           batch_normalization_70[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_74 (Conv2D)              (None, 8, 8, 64)     36864       activation_69[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_71 (BatchNo (None, 8, 8, 64)     256         conv2d_74[0][0]                  \n","__________________________________________________________________________________________________\n","activation_70 (Activation)      (None, 8, 8, 64)     0           batch_normalization_71[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_75 (Conv2D)              (None, 8, 8, 256)    16384       activation_70[0][0]              \n","__________________________________________________________________________________________________\n","add_21 (Add)                    (None, 8, 8, 256)    0           conv2d_75[0][0]                  \n","                                                                 add_20[0][0]                     \n","__________________________________________________________________________________________________\n","batch_normalization_72 (BatchNo (None, 8, 8, 256)    1024        add_21[0][0]                     \n","__________________________________________________________________________________________________\n","activation_71 (Activation)      (None, 8, 8, 256)    0           batch_normalization_72[0][0]     \n","__________________________________________________________________________________________________\n","average_pooling2d_3 (AveragePoo (None, 1, 1, 256)    0           activation_71[0][0]              \n","__________________________________________________________________________________________________\n","flatten_3 (Flatten)             (None, 256)          0           average_pooling2d_3[0][0]        \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 2)            514         flatten_3[0][0]                  \n","__________________________________________________________________________________________________\n","activation_72 (Activation)      (None, 2)            0           dense_3[0][0]                    \n","==================================================================================================\n","Total params: 294,766\n","Trainable params: 291,176\n","Non-trainable params: 3,590\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]}]}